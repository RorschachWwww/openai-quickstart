{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4be2e6fa-2187-4617-8433-0db4fb0c099c",
   "metadata": {},
   "source": [
    "# LangChain 核心模块学习：Model I/O\n",
    "\n",
    "`Model I/O` 是 LangChain 为开发者提供的一套面向 LLM 的标准化模型接口，包括模型输入（Prompts）、模型输出（Output Parsers）和模型本身（Models）。\n",
    "\n",
    "- Prompts：模板化、动态选择和管理模型输入\n",
    "- Models：以通用接口调用语言模型\n",
    "- Output Parser：从模型输出中提取信息，并规范化内容\n",
    "\n",
    "![](../images/model_io.jpeg)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe06fea2-88a5-4036-a9f6-69c27b8363cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain_community in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain_openai in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (0.2.10)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (3.11.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (2.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain_openai) (1.55.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain_openai) (0.8.0)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.68.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.0.1-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from chromadb) (0.28.0)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: sniffio in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.17.0-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.0-cp310-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-14.1-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\tools\\miniconda\\envs\\langchain_0_2\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n",
      "   ---------------------------------------- 0.0/617.9 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/617.9 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 617.9/617.9 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
      "Downloading grpcio-1.68.0-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.4/4.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 11.5 MB/s eta 0:00:00\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached mmh3-5.0.1-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading onnxruntime-1.20.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.3 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.5/11.3 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.3 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.3 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n",
      "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.4/2.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 9.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.14.0-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.0-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading watchfiles-1.0.0-cp310-none-win_amd64.whl (285 kB)\n",
      "Downloading websockets-14.1-cp310-cp310-win_amd64.whl (163 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.0-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pypika, mpmath, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, sympy, shellingham, pyreadline3, pyproject_hooks, pyasn1, protobuf, opentelemetry-util-http, oauthlib, mmh3, mdurl, importlib-resources, httptools, grpcio, fsspec, filelock, click, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, humanfriendly, huggingface-hub, googleapis-common-protos, deprecated, build, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, fastapi, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.20 click-8.1.7 coloredlogs-15.0.1 deprecated-1.2.15 durationpy-0.9 fastapi-0.115.5 filelock-3.16.1 flatbuffers-24.3.25 fsspec-2024.10.0 google-auth-2.36.0 googleapis-common-protos-1.66.0 grpcio-1.68.0 httptools-0.6.4 huggingface-hub-0.26.3 humanfriendly-10.0 importlib-metadata-8.5.0 importlib-resources-6.4.5 kubernetes-31.0.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 opentelemetry-util-http-0.49b2 posthog-3.7.4 protobuf-5.29.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rich-13.9.4 rsa-4.9 shellingham-1.5.4 starlette-0.41.3 sympy-1.13.3 tokenizers-0.21.0 typer-0.14.0 uvicorn-0.32.1 watchfiles-1.0.0 websockets-14.1 wrapt-1.17.0 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain langchain_community langchain_openai chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a2474-0b69-4830-85cd-3715c22df304",
   "metadata": {},
   "source": [
    "## 模型输入 Prompts\n",
    "\n",
    "一个语言模型的提示是用户提供的一组指令或输入，用于引导模型的响应，帮助它理解上下文并生成相关和连贯的基于语言的输出，例如回答问题、完成句子或进行对话。\n",
    "\n",
    "\n",
    "- 提示模板（Prompt Templates）：参数化的模型输入\n",
    "- 示例选择器（Example Selectors）：动态选择要包含在提示中的示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14f4cf-8e30-47ab-b8b1-d58a90b5b1c1",
   "metadata": {},
   "source": [
    "## 提示模板 Prompt Templates\n",
    "\n",
    "**Prompt Templates 提供了一种预定义、动态注入、模型无关和参数化的提示词生成方式，以便在不同的语言模型之间重用模板。**\n",
    "\n",
    "一个模板可能包括指令、少量示例以及适用于特定任务的具体背景和问题。\n",
    "\n",
    "通常，提示要么是一个字符串（LLMs），要么是一组聊天消息（Chat Model）。\n",
    "\n",
    "\n",
    "类继承关系:\n",
    "\n",
    "```\n",
    "BasePromptTemplate --> PipelinePromptTemplate\n",
    "                       StringPromptTemplate --> PromptTemplate\n",
    "                                                FewShotPromptTemplate\n",
    "                                                FewShotPromptWithTemplates\n",
    "                       BaseChatPromptTemplate --> AutoGPTPrompt\n",
    "                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "BaseMessagePromptTemplate --> MessagesPlaceholder\n",
    "                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\n",
    "                                                                  HumanMessagePromptTemplate\n",
    "                                                                  AIMessagePromptTemplate\n",
    "                                                                  SystemMessagePromptTemplate\n",
    "\n",
    "PromptValue --> StringPromptValue\n",
    "                ChatPromptValue\n",
    "```\n",
    "\n",
    "\n",
    "**代码实现：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/prompts**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c4a43-530b-4ee1-af40-e2c90c0c8c66",
   "metadata": {},
   "source": [
    "### 使用 PromptTemplate 类生成提升词\n",
    "\n",
    "**通常，`PromptTemplate` 类的实例，使用Python的`str.format`语法生成模板化提示；也可以使用其他模板语法（例如jinja2）。**\n",
    "\n",
    "#### 使用 from_template 方法实例化 PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d681566-cde1-4ae5-8cd7-f53cf59c3e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\tools\\Miniconda\\envs\\langchain_0_2\\python.exe\n",
      "Tell me a funny joke about ducks.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "# 使用 format 生成提示\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"ducks\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09e2b7f-d1e1-4cc3-bf9b-22de07df5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['adjective', 'content'] input_types={} partial_variables={} template='Tell me a {adjective} joke about {content}.'\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c329075-0a6a-4d17-9cc3-081be37f7917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a joke\"\n",
    ")\n",
    "# 生成提示\n",
    "prompt = prompt_template.format()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13696d97-9c8f-4530-92ec-a9cdf2012bf7",
   "metadata": {},
   "source": [
    "#### 使用构造函数（Initializer）实例化 PromptTemplate\n",
    "\n",
    "使用构造函数实例化 `prompt_template` 时必须传入参数：`input_variables` 和 `template`。\n",
    "\n",
    "在生成提示过程中，会检查输入变量与模板字符串中的变量是否匹配，如果不匹配，则会引发异常；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e7686a-dcca-4410-9263-6e941bee9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"],\n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39cd8e-99a9-4d57-aff9-0cbc673336df",
   "metadata": {},
   "source": [
    "传入 content 后才能生成可用的 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49767dc4-2057-4fee-8fb0-24436194c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"],\n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516cee06-9673-4c9d-9499-1355034ad82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['adjective', 'content'] input_types={} partial_variables={} template='Tell me a {adjective} joke about {content}.'\n"
     ]
    }
   ],
   "source": [
    "print(valid_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c7a677-4ae8-4c81-829c-49fd597c45bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb638c-7033-4cf2-9354-6481170c9892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754269b1-a2a7-487e-85b3-3be916be581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"讲{num}个给程序员听得笑话\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3de140d-1a5e-450b-a8c8-5350d74b630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-svcacct-5ca6wZIAW7UdqaGltNHPZfgd9LSPGE893-tdOn-ElRriYrhvlmTddVKLg5JuT3BlbkFJ32gwDfptEC-Gt9IdjkPmVqh6u1JRgsxcsOQPa7bHvp1ohQik2XsFNJ8iXKEA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc708563-8556-466d-9ea5-9f41c8e6a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 讲2个给程序员听得笑话\n",
      "result: \n",
      "\n",
      "1. 程序员和产品经理一起去旅游，走在路上看到一只牛，产品经理说：“这是一只牛，我们可以把它卖给农民，收入就能提高了。”程序员回答：“这是一只牛，我们可以用它来帮我们写代码，我们的效率就能提高了。”\n",
      "\n",
      "2. 有一天，一个程序员在路上遇到了一只兔子，兔子说：我正在学习如何写代码，但是我不知道从哪里开始。程序员回答：你应该先学习如何跳跃，这样你就可以跳过所有的bug了。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", max_tokens=1000)\n",
    "\n",
    "prompt = prompt_template.format(num=2)\n",
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6dd3ea-084a-4426-bba0-2676d42842c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. 为什么程序员喜欢用黑色背景的编辑器？\n",
      "因为黑色背景可以减少眼睛的视觉疲劳，让程序员可以更长时间地专注于代码编写，而不用频繁休息。\n",
      "\n",
      "2. 为什么程序员喜欢喝咖啡？\n",
      "因为咖啡可以提神，让程序员保持清醒的大脑思考能力，从而更有效率地编写代码。\n",
      "\n",
      "3. 为什么程序员不喜欢演讲？\n",
      "因为他们总是习惯把所有东西都写在代码里，而不是用口头语言表达。当需要做演讲时，他们就会感到不知所措。\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt_template.format(num=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890a0b3-c5af-4d0e-b1a1-3439f3199edc",
   "metadata": {},
   "source": [
    "#### 使用 jinja2 生成模板化提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e98620d-3462-4a7d-b9ef-1a9a0fd1f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jinja2_template = \"Tell me a {{ adjective }} joke about {{ content }}\"\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "\n",
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b707a3f-c6c1-4aa1-9ad2-4067c0430e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['adjective', 'content'] input_types={} partial_variables={} template='Tell me a {{ adjective }} joke about {{ content }}' template_format='jinja2'\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c08d8-14ee-459e-9a5c-f0d659d08d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b48d41-608c-46e4-ac05-882b05cbfec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c34c3078-3ece-4708-8bf8-cdc02968ea70",
   "metadata": {},
   "source": [
    "#### 实测：生成多种编程语言版本的快速排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c73de848-8091-4ebd-9127-ea33582bad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_prompt_template = PromptTemplate.from_template(\n",
    "    \"生成可执行的快速排序 {programming_language} 代码\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c2ee81-80aa-4ad9-b936-8c2e1b2ba27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#\n",
      "\n",
      "def quick_sort(arr):\n",
      "    # 使用递归的方式实现快排\n",
      "    # 首先选择一个基准值，将数组分为小于等于基准值和大于基准值的两部分\n",
      "    # 然后再分别对这两部分进行快排，最后将两部分合并即可得到有序数组\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]  # 将第一个元素作为基准值\n",
      "        less, greater = [], []\n",
      "        for x in arr[1:]:\n",
      "            if x <= pivot:\n",
      "                less.append(x)\n",
      "            else:\n",
      "                greater.append(x)\n",
      "        return quick_sort(less) + [pivot] + quick_sort(greater)\n",
      "\n",
      "# 测试代码\n",
      "print(quick_sort([3, 2, 1]))\n",
      "print(quick_sort([9, 2, 5, 6, 1, 0, 8]))\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(sort_prompt_template.format(programming_language=\"python\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3b370e-a647-4913-b4a0-c33d4dd11e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "public class QuickSort {\n",
      "    public static void main(String[] args) {\n",
      "        int[] arr = {3, 5, 8, 1, 2, 9, 4, 7, 6};\n",
      "        quickSort(arr, 0, arr.length - 1);\n",
      "        for (int num : arr) {\n",
      "            System.out.print(num + \" \");\n",
      "        }\n",
      "    }\n",
      "\n",
      "    public static void quickSort(int[] arr, int left, int right) {\n",
      "        if (left >= right) {\n",
      "            return;\n",
      "        }\n",
      "        int pivot = partition(arr, left, right);\n",
      "        quickSort(arr, left, pivot - 1);\n",
      "        quickSort(arr, pivot + 1, right);\n",
      "    }\n",
      "\n",
      "    public static int partition(int[] arr, int left, int right) {\n",
      "        int pivot = arr[left];\n",
      "        while (left < right) {\n",
      "            while (left < right && arr[right] >= pivot) {\n",
      "                right--;\n",
      "            }\n",
      "            arr[left] = arr[right];\n",
      "            while (left < right && arr[left] <= pivot) {\n",
      "                left++;\n",
      "            }\n",
      "            arr[right] = arr[left];\n",
      "        }\n",
      "        arr[left] = pivot;\n",
      "        return left;\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(sort_prompt_template.format(programming_language=\"java\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ef61ce-af9f-40d6-970e-cdacc4d4736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。\n",
      "\n",
      "首先我们需要一个划分函数，它的作用是将数组分成两部分，一部分的元素比另一部分的元素小，然后将两部分分别排序。这里我们采用数组的第一个元素作为划分元素，遍历数组，将小于等于划分元素的元素放到数组的左边，将大于划分元素的元素放到数组的右边。\n",
      "\n",
      "划分函数的代码如下所示：\n",
      "\n",
      "```c++\n",
      "int partition(vector<int>& nums, int low, int high)\n",
      "{\n",
      "    int pivot = nums[low]; // 选择第一个元素作为划分元素\n",
      "    int i = low, j = high + 1;\n",
      "    while (true) {\n",
      "        // 从左往右找到第一个大于等于划分元素的元素\n",
      "        while (i < high && nums[++i] <= pivot);\n",
      "        // 从右往左找到第一个小于等于划分元素的元素\n",
      "        while (j > low && nums[--j] >= pivot);\n",
      "        if (i >= j) break;\n",
      "        // 将两个元素交换\n",
      "        swap(nums[i], nums[j]);\n",
      "    }\n",
      "    // 将划分元素放到正确的位置上\n",
      "    swap(nums[low], nums[j]);\n",
      "    return j;\n",
      "}\n",
      "```\n",
      "\n",
      "然后我们就可以编写快速排序的函数了，它的作用是对数组的指定范围进行排序。这里我们采用递归的方法，先对数组的前半部分排序，再对后半部分排序，最后将两部分合并起来。快速排序的代码如下所示：\n",
      "\n",
      "```c++\n",
      "void quick_sort(vector<int>& nums, int low, int high)\n",
      "{\n",
      "    // 递归终止条件\n",
      "    if (low >= high) return;\n",
      "\n",
      "    // 划分数组\n",
      "    int pivot_index = partition(nums, low, high);\n",
      "\n",
      "    // 分别对左右两部分递归排序\n",
      "    quick_sort(nums, low, pivot_index - 1);\n",
      "    quick_sort(nums, pivot_index + 1, high);\n",
      "}\n",
      "```\n",
      "\n",
      "最后我们只需要调用 `quick_sort` 函数，就可以对整个数组进行排序了。完整的代码如下所示：\n",
      "\n",
      "```c++\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "using namespace std;\n",
      "\n",
      "int partition(vector<int>& nums, int low, int high)\n",
      "{\n",
      "    int pivot = nums[low]; // 选择第一个元素作为划分元素\n",
      "    int i = low, j = high + 1;\n",
      "    while (true) {\n",
      "        // 从左往右找到第一个大于等于划分元素的元素\n",
      "        while (i < high && nums[++i] <= pivot);\n",
      "        // 从右往左找到第一个小于等于划分元素的元素\n",
      "        while (j > low && nums[--j] >= pivot);\n",
      "        if (i >= j) break;\n",
      "        // 将两个元素交换\n",
      "        swap(nums[i], nums[j]);\n",
      "    }\n",
      "    // 将划分元素放到正确的位置上\n",
      "    swap(nums[low], nums[j]);\n",
      "    return j;\n",
      "}\n",
      "\n",
      "void quick_sort(vector<int>& nums, int low, int high)\n",
      "{\n",
      "    // 递归终止条件\n",
      "    if (low >= high) return;\n",
      "\n",
      "    // 划分数组\n",
      "    int pivot_index = partition(nums, low, high);\n",
      "\n",
      "    // 分别对左右两部分递归排序\n",
      "    quick_sort(nums, low, pivot_index - 1);\n",
      "    quick_sort(nums, pivot_index + 1, high);\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    // 输入数组\n",
      "    vector<int> nums = {3, 6, 1, 5, 7, 2, 4};\n",
      "    // 对数组排序\n",
      "    quick_sort(nums, 0, nums.size() - 1);\n",
      "    // 输出结果\n",
      "    for (int num : nums) {\n",
      "        cout << num << \" \";\n",
      "    }\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "## 总结\n",
      "\n",
      "快速排序是一种高效的排序算法，它的时间复杂度为 O(nlogn)，空间复杂度为 O(1)。它的核心思想是\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(sort_prompt_template.format(programming_language=\"C++\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510954a1-2c16-414c-ada1-af9d439d43be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "621af4b1-0337-428a-bc88-17f1ff1f876e",
   "metadata": {},
   "source": [
    "## 使用 ChatPromptTemplate 类生成适用于聊天模型的聊天记录\n",
    "\n",
    "**`ChatPromptTemplate` 类的实例，使用`format_messages`方法生成适用于聊天模型的提示。**\n",
    "\n",
    "### 使用 from_messages 方法实例化 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875c8534-7317-4111-9658-80a926458168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "# 生成提示\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceb1c89b-c71a-4e7c-bf81-2f2a5ddcf80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99257dd-3dad-48bd-9c75-1ab9348179ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI bot. Your name is Bob.\n",
      "What is your name?\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)\n",
    "print(messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feba7a9c-d3c3-418c-8ee7-b033c3da1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40342d2c-43f1-4bce-ae06-0dda2fdea608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 50, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e7a243cc-d54b-4c37-a325-ea35443ef6b6-0', usage_metadata={'input_tokens': 50, 'output_tokens': 12, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a095b3-3e0d-4a05-9cea-620a6155be22",
   "metadata": {},
   "source": [
    "### 摘要总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe696871-bc41-48f3-b41b-43bb9f9ddcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你将获得关于同一主题的{num}篇文章（用-----------标签分隔）。首先总结每篇文章的论点。然后指出哪篇文章提出了更好的论点，并解释原因。\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1d1c65-fd90-48d0-88eb-3eb1cade5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = summary_template.format_messages(\n",
    "    num=3,\n",
    "    user_input='''1. [PHP是世界上最好的语言]\n",
    "PHP是世界上最好的情感派编程语言，无需逻辑和算法，只要情绪。它能被蛰伏在冰箱里的PHP大神轻易驾驭，会话结束后的感叹号也能传达对代码的热情。写PHP就像是在做披萨，不需要想那么多，只需把配料全部扔进一个碗，然后放到服务器上，热乎乎出炉的网页就好了。\n",
    "-----------\n",
    "2. [Python是世界上最好的语言]\n",
    "Python是世界上最好的拜金主义者语言。它坚信：美丽就是力量，简洁就是灵魂。Python就像是那个永远在你皱眉的那一刻扔给你言情小说的好友。只有Python，你才能够在两行代码之间感受到飘逸的花香和清新的微风。记住，这世上只有一种语言可以使用空格来领导全世界的进步，那就是Python。\n",
    "-----------\n",
    "3. [Java是世界上最好的语言]\n",
    "Java是世界上最好的德育课编程语言，它始终坚守了严谨、安全的编程信条。Java就像一个严格的老师，他不会对你怀柔，不会让你偷懒，也不会让你走捷径，但他教会你规范和自律。Java就像是那个喝咖啡也算加班费的上司，拥有对邪恶的深度厌恶和对善良的深度拥护。\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0042c49-3a75-4098-9037-eb54f288b55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [PHP是世界上最好的语言]\n",
      "PHP是世界上最好的情感派编程语言，无需逻辑和算法，只要情绪。它能被蛰伏在冰箱里的PHP大神轻易驾驭，会话结束后的感叹号也能传达对代码的热情。写PHP就像是在做披萨，不需要想那么多，只需把配料全部扔进一个碗，然后放到服务器上，热乎乎出炉的网页就好了。\n",
      "-----------\n",
      "2. [Python是世界上最好的语言]\n",
      "Python是世界上最好的拜金主义者语言。它坚信：美丽就是力量，简洁就是灵魂。Python就像是那个永远在你皱眉的那一刻扔给你言情小说的好友。只有Python，你才能够在两行代码之间感受到飘逸的花香和清新的微风。记住，这世上只有一种语言可以使用空格来领导全世界的进步，那就是Python。\n",
      "-----------\n",
      "3. [Java是世界上最好的语言]\n",
      "Java是世界上最好的德育课编程语言，它始终坚守了严谨、安全的编程信条。Java就像一个严格的老师，他不会对你怀柔，不会让你偷懒，也不会让你走捷径，但他教会你规范和自律。Java就像是那个喝咖啡也算加班费的上司，拥有对邪恶的深度厌恶和对善良的深度拥护。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fee264c1-941b-42b5-a737-9320eb4f5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92ada5fc-bee1-4e66-9f87-7d454491108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一篇文章认为PHP是世界上最好的语言，强调了它的情感化特点和使用的简单性；第二篇文章认为Python是最好的语言，突出了它的简洁和美感；第三篇文章则认为Java是最好的语言，强调了它的严谨和安全性。\n",
      "\n",
      "在这三篇文章中，第三篇关于Java的论点提出了更好的论点。尽管这三篇文章的论点都带有一定的幽默和夸张，但第三篇文章关于Java的论点更加严肃和实际。它强调了Java的规范性和自律性，这是在编程中非常重要的品质。与PHP和Python相比，Java更注重代码的安全性和可靠性，这些特点使得Java在大型企业应用和系统开发中更为受欢迎。因此，第三篇文章提出的论点更具有说服力和实用性。\n"
     ]
    }
   ],
   "source": [
    "print(chat_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc089de4-84e1-4d50-91e5-6d49ade2cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你将获得关于同一主题的2篇文章（用-----------标签分隔）。首先总结每篇文章的论点。然后指出哪篇文章提出了更好的论点，并解释原因。', additional_kwargs={}, response_metadata={}), HumanMessage(content='1.认为“道可道”中的第一个“道”，指的是道理，如仁义礼智之类；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，所谓“道可道，非常道”，指的是可以言说的道理，不是恒久存在的“道”，恒久存在的“道”不可言说。如苏辙说：“莫非道也。而可道者不可常，惟不可道，而后可常耳。今夫仁义礼智，此道之可道者也。然而仁不可以为义，而礼不可以为智，可道之不可常如此。……而道常不变，不可道之能常如此。”蒋锡昌说：“此道为世人所习称之道，即今人所谓‘道理’也，第一‘道’字应从是解。《广雅·释诂》二：‘道，说也’，第二‘道’字应从是解。‘常’乃真常不易之义，在文法上为区别词。……第三‘道’字即二十五章‘道法自然’之‘道’，……乃老子学说之总名也”。陈鼓应说：“第一个‘道’字是人们习称之道，即今人所谓‘道理’。第二个‘道’字，是指言说的意思。第三个‘道’字，是老子哲学上的专有名词，在本章它意指构成宇宙的实体与动力。……‘常道’之‘常’，为真常、永恒之意。……可以用言词表达的道，就不是常道”。\\n-----------\\n2.认为“道可道”中的第一个“道”，指的是宇宙万物的本原；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，“道可道，非常道”，指可以言说的“道”，就不是恒久存在的“道”。如张默生说：“‘道’，指宇宙的本体而言。……‘常’，是经常不变的意思。……可以说出来的道，便不是经常不变的道”。董平说：“第一个‘道’字与‘可道’之‘道’，内涵并不相同。第一个‘道’字，是老子所揭示的作为宇宙本根之‘道’；‘可道’之‘道’，则是‘言说’的意思。……这里的大意就是说：凡一切可以言说之‘道’，都不是‘常道’或永恒之‘道’”。汤漳平等说：“第一句中的三个‘道’，第一、三均指形上之‘道’，中间的‘道’作动词，为可言之义。……道可知而可行，但非恒久不变之道”。\\n--------\\n3.认为“道可道”中的第一个“道”，指的是宇宙万物的本原；“可道”中的“道”，指言说的意思；“常道”，则指的是平常人所讲之道、常俗之道。因此，“道可道，非常道”，指“道”是可以言说的，但它不是平常人所谓的道或常俗之道。如李荣说：“道者，虚极之理也。夫论虚极之理，不可以有无分其象，不可以上下格其真。……圣人欲坦兹玄路，开以教门，借圆通之名，目虚极之理，以理可名，称之可道。故曰‘吾不知其名，字之曰道’。非常道者，非是人间常俗之道也。人间常俗之道，贵之以礼义，尚之以浮华，丧身以成名，忘己而徇利。”司马光说：“世俗之谈道者，皆曰道体微妙，不可名言。老子以为不然，曰道亦可言道耳，然非常人之所谓道也。……常人之所谓道者，凝滞于物。”裘锡圭说：“到目前为止，可以说，几乎从战国开始，大家都把‘可道’之‘道’……看成老子所否定的，把‘常道’‘常名’看成老子所肯定的。这种看法其实有它不合理的地方，……‘道’是可以说的。《老子》这个《道经》第一章，开宗明义是要讲他的‘道’。第一个‘道’字，理所应当，也是讲他要讲的‘道’：道是可以言说的。……那么这个‘恒’字应该怎么讲？我认为很简单，‘恒’字在古代作定语用，经常是‘平常’‘恒常’的意思。……‘道’是可以言说的，但是我要讲的这个‘道’，不是‘恒道’，它不是一般人所讲的‘道’。\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = summary_template.format_messages(\n",
    "    num=2,\n",
    "    user_input='''1.认为“道可道”中的第一个“道”，指的是道理，如仁义礼智之类；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，所谓“道可道，非常道”，指的是可以言说的道理，不是恒久存在的“道”，恒久存在的“道”不可言说。如苏辙说：“莫非道也。而可道者不可常，惟不可道，而后可常耳。今夫仁义礼智，此道之可道者也。然而仁不可以为义，而礼不可以为智，可道之不可常如此。……而道常不变，不可道之能常如此。”蒋锡昌说：“此道为世人所习称之道，即今人所谓‘道理’也，第一‘道’字应从是解。《广雅·释诂》二：‘道，说也’，第二‘道’字应从是解。‘常’乃真常不易之义，在文法上为区别词。……第三‘道’字即二十五章‘道法自然’之‘道’，……乃老子学说之总名也”。陈鼓应说：“第一个‘道’字是人们习称之道，即今人所谓‘道理’。第二个‘道’字，是指言说的意思。第三个‘道’字，是老子哲学上的专有名词，在本章它意指构成宇宙的实体与动力。……‘常道’之‘常’，为真常、永恒之意。……可以用言词表达的道，就不是常道”。\n",
    "-----------\n",
    "2.认为“道可道”中的第一个“道”，指的是宇宙万物的本原；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，“道可道，非常道”，指可以言说的“道”，就不是恒久存在的“道”。如张默生说：“‘道’，指宇宙的本体而言。……‘常’，是经常不变的意思。……可以说出来的道，便不是经常不变的道”。董平说：“第一个‘道’字与‘可道’之‘道’，内涵并不相同。第一个‘道’字，是老子所揭示的作为宇宙本根之‘道’；‘可道’之‘道’，则是‘言说’的意思。……这里的大意就是说：凡一切可以言说之‘道’，都不是‘常道’或永恒之‘道’”。汤漳平等说：“第一句中的三个‘道’，第一、三均指形上之‘道’，中间的‘道’作动词，为可言之义。……道可知而可行，但非恒久不变之道”。\n",
    "--------\n",
    "3.认为“道可道”中的第一个“道”，指的是宇宙万物的本原；“可道”中的“道”，指言说的意思；“常道”，则指的是平常人所讲之道、常俗之道。因此，“道可道，非常道”，指“道”是可以言说的，但它不是平常人所谓的道或常俗之道。如李荣说：“道者，虚极之理也。夫论虚极之理，不可以有无分其象，不可以上下格其真。……圣人欲坦兹玄路，开以教门，借圆通之名，目虚极之理，以理可名，称之可道。故曰‘吾不知其名，字之曰道’。非常道者，非是人间常俗之道也。人间常俗之道，贵之以礼义，尚之以浮华，丧身以成名，忘己而徇利。”司马光说：“世俗之谈道者，皆曰道体微妙，不可名言。老子以为不然，曰道亦可言道耳，然非常人之所谓道也。……常人之所谓道者，凝滞于物。”裘锡圭说：“到目前为止，可以说，几乎从战国开始，大家都把‘可道’之‘道’……看成老子所否定的，把‘常道’‘常名’看成老子所肯定的。这种看法其实有它不合理的地方，……‘道’是可以说的。《老子》这个《道经》第一章，开宗明义是要讲他的‘道’。第一个‘道’字，理所应当，也是讲他要讲的‘道’：道是可以言说的。……那么这个‘恒’字应该怎么讲？我认为很简单，‘恒’字在古代作定语用，经常是‘平常’‘恒常’的意思。……‘道’是可以言说的，但是我要讲的这个‘道’，不是‘恒道’，它不是一般人所讲的‘道’。\n",
    "'''\n",
    ")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c048f55c-72f7-463d-af87-9aca561ddf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一篇文章的论点是：《道德经》中的“道可道，非常道”表明可以言说的道理并不是恒久存在的道，恒久存在的道不可言说。作者引用了苏辙、蒋锡昌和陈鼓应的观点来支持自己的解释。\n",
      "\n",
      "第二篇文章的论点是：同样地，《道德经》中的“道可道，非常道”指的是可以言说的道并不是恒久存在的道，即宇宙的本原。作者引用了张默生、董平和汤漳平等人的观点来支撑自己的论点。\n",
      "\n",
      "第一篇文章提出了更好的论点。这是因为它通过引用不同学者的观点，对“道可道，非常道”的含义进行了详细解释，强调了可以言说的道理和恒久存在的道之间的区别。文章中的引用内容也更具体和有说服力，使读者更容易理解作者的论点。第一篇文章的论证更加清晰和连贯，论述更加深入，因此更具有说服力。\n"
     ]
    }
   ],
   "source": [
    "chat_result = chat_model.invoke(messages)\n",
    "print(chat_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211d7fea-ff97-4c92-a478-f14be0cbd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = summary_template.format_messages(\n",
    "    num=2,\n",
    "    user_input='''1.认为“道可道”中的第一个“道”，指的是道理，如仁义礼智之类；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，所谓“道可道，非常道”，指的是可以言说的道理，不是恒久存在的“道”，恒久存在的“道”不可言说。如苏辙说：“莫非道也。而可道者不可常，惟不可道，而后可常耳。今夫仁义礼智，此道之可道者也。然而仁不可以为义，而礼不可以为智，可道之不可常如此。……而道常不变，不可道之能常如此。”蒋锡昌说：“此道为世人所习称之道，即今人所谓‘道理’也，第一‘道’字应从是解。《广雅·释诂》二：‘道，说也’，第二‘道’字应从是解。‘常’乃真常不易之义，在文法上为区别词。……第三‘道’字即二十五章‘道法自然’之‘道’，……乃老子学说之总名也”。陈鼓应说：“第一个‘道’字是人们习称之道，即今人所谓‘道理’。第二个‘道’字，是指言说的意思。第三个‘道’字，是老子哲学上的专有名词，在本章它意指构成宇宙的实体与动力。……‘常道’之‘常’，为真常、永恒之意。……可以用言词表达的道，就不是常道”。\n",
    "-----------\n",
    "2.认为“道可道”中的第一个“道”，指的是宇宙万物的本原；“可道”中的“道”，指言说的意思；“常道”，指恒久存在的“道”。因此，“道可道，非常道”，指可以言说的“道”，就不是恒久存在的“道”。如张默生说：“‘道’，指宇宙的本体而言。……‘常’，是经常不变的意思。……可以说出来的道，便不是经常不变的道”。董平说：“第一个‘道’字与‘可道’之‘道’，内涵并不相同。第一个‘道’字，是老子所揭示的作为宇宙本根之‘道’；‘可道’之‘道’，则是‘言说’的意思。……这里的大意就是说：凡一切可以言说之‘道’，都不是‘常道’或永恒之‘道’”。汤漳平等说：“第一句中的三个‘道’，第一、三均指形上之‘道’，中间的‘道’作动词，为可言之义。……道可知而可行，但非恒久不变之道”。\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c524c96a-a754-47fd-bfc0-45aafd642d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一篇文章认为“道可道”中的第一个“道”指的是道理，第二个“道”指言说的意思，而“常道”指的是恒久存在的道。因此，“道可道，非常道”表明可以言说的道理并非恒久存在的道。苏辙、蒋锡昌、陈鼓应都支持这一观点。\n",
      "\n",
      "第二篇文章则认为“道可道”中的第一个“道”指的是宇宙万物的本原，第二个“道”指言说的意思，而“常道”指恒久存在的道。根据这个解释，“道可道，非常道”表明可以言说的道并非恒久存在的道。张默生、董平、汤漳平等都支持这一解读。\n",
      "\n",
      "在这两篇文章中，第一篇提出了更好的论点。这是因为第一篇文章更加详细地解释了每个“道”所代表的含义，并引用了多位学者的观点来支持论点。这种详细解释和引用权威意见的方式使得第一篇文章的论点更加有说服力和可信度。\n"
     ]
    }
   ],
   "source": [
    "chat_result = chat_model.invoke(messages)\n",
    "print(chat_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6280a-f38d-46f8-a5ff-06f98025f46e",
   "metadata": {},
   "source": [
    "### 使用 FewShotPromptTemplate 类生成 Few-shot Prompt \n",
    "\n",
    "构造 few-shot prompt 的方法通常有两种：\n",
    "- 从示例集（set of examples）中手动选择；\n",
    "- 通过示例选择器（Example Selector）自动选择."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d086861b-b576-446e-bf2f-89544e500c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"谁活得更久，穆罕默德·阿里还是艾伦·图灵？\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：穆罕默德·阿里去世时多大了？\n",
    "中间答案：穆罕默德·阿里去世时74岁。\n",
    "追问：艾伦·图灵去世时多大了？\n",
    "中间答案：艾伦·图灵去世时41岁。\n",
    "所以最终答案是：穆罕默德·阿里\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"craigslist的创始人是什么时候出生的？\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是craigslist的创始人？\n",
    "中间答案：Craigslist是由Craig Newmark创办的。\n",
    "追问：Craig Newmark是什么时候出生的？\n",
    "中间答案：Craig Newmark出生于1952年12月6日。\n",
    "所以最终答案是：1952年12月6日\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"乔治·华盛顿的外祖父是谁？\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是乔治·华盛顿的母亲？\n",
    "中间答案：乔治·华盛顿的母亲是Mary Ball Washington。\n",
    "追问：Mary Ball Washington的父亲是谁？\n",
    "中间答案：Mary Ball Washington的父亲是Joseph Ball。\n",
    "所以最终答案是：Joseph Ball\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"《大白鲨》和《皇家赌场》的导演是同一个国家的吗？\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "这里需要进一步的问题吗：是的。\n",
    "追问：谁是《大白鲨》的导演？\n",
    "中间答案：《大白鲨》的导演是Steven Spielberg。\n",
    "追问：Steven Spielberg来自哪里？\n",
    "中间答案：美国。\n",
    "追问：谁是《皇家赌场》的导演？\n",
    "中间答案：《皇家赌场》的导演是Martin Campbell。\n",
    "追问：Martin Campbell来自哪里？\n",
    "中间答案：新西兰。\n",
    "所以最终答案是：不是\n",
    "\"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a722158-fc50-4d34-827e-224e9f6e5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 谁活得更久，穆罕默德·阿里还是艾伦·图灵？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：穆罕默德·阿里去世时多大了？\n",
      "中间答案：穆罕默德·阿里去世时74岁。\n",
      "追问：艾伦·图灵去世时多大了？\n",
      "中间答案：艾伦·图灵去世时41岁。\n",
      "所以最终答案是：穆罕默德·阿里\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "# **examples[0] 是将examples[0] 字典的键值对（question-answer）解包并传递给format，作为函数参数\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4b8892e-c8a8-4c58-89be-5e331f040a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['answer', 'question'] input_types={} partial_variables={} template='Question: {question}\\n{answer}'\n"
     ]
    }
   ],
   "source": [
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c5861c-a383-4bd5-a3ee-27258c774fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 《大白鲨》和《皇家赌场》的导演是同一个国家的吗？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是《大白鲨》的导演？\n",
      "中间答案：《大白鲨》的导演是Steven Spielberg。\n",
      "追问：Steven Spielberg来自哪里？\n",
      "中间答案：美国。\n",
      "追问：谁是《皇家赌场》的导演？\n",
      "中间答案：《皇家赌场》的导演是Martin Campbell。\n",
      "追问：Martin Campbell来自哪里？\n",
      "中间答案：新西兰。\n",
      "所以最终答案是：不是\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_prompt.format(**examples[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a053e2-c8e0-4cd8-9648-01a00057d1b5",
   "metadata": {},
   "source": [
    "#### 关于解包的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32806357-32c4-4e1c-ad6c-8b664bbf3004",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 谁活得更久，穆罕默德·阿里还是艾伦·图灵？\n",
      "Answer: \n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：穆罕默德·阿里去世时多大了？\n",
      "中间答案：穆罕默德·阿里去世时74岁。\n",
      "追问：艾伦·图灵去世时多大了？\n",
      "中间答案：艾伦·图灵去世时41岁。\n",
      "所以最终答案是：穆罕默德·阿里\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_info(question, answer):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "print_info(**examples[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cc0de-1065-4997-ad78-b3ad4325585c",
   "metadata": {},
   "source": [
    "### 生成 Few-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab9d0a1c-ca67-4b97-9095-011102c06046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 谁活得更久，穆罕默德·阿里还是艾伦·图灵？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：穆罕默德·阿里去世时多大了？\n",
      "中间答案：穆罕默德·阿里去世时74岁。\n",
      "追问：艾伦·图灵去世时多大了？\n",
      "中间答案：艾伦·图灵去世时41岁。\n",
      "所以最终答案是：穆罕默德·阿里\n",
      "\n",
      "\n",
      "Question: craigslist的创始人是什么时候出生的？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是craigslist的创始人？\n",
      "中间答案：Craigslist是由Craig Newmark创办的。\n",
      "追问：Craig Newmark是什么时候出生的？\n",
      "中间答案：Craig Newmark出生于1952年12月6日。\n",
      "所以最终答案是：1952年12月6日\n",
      "\n",
      "\n",
      "Question: 乔治·华盛顿的外祖父是谁？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是乔治·华盛顿的母亲？\n",
      "中间答案：乔治·华盛顿的母亲是Mary Ball Washington。\n",
      "追问：Mary Ball Washington的父亲是谁？\n",
      "中间答案：Mary Ball Washington的父亲是Joseph Ball。\n",
      "所以最终答案是：Joseph Ball\n",
      "\n",
      "\n",
      "Question: 《大白鲨》和《皇家赌场》的导演是同一个国家的吗？\n",
      "\n",
      "这里需要进一步的问题吗：是的。\n",
      "追问：谁是《大白鲨》的导演？\n",
      "中间答案：《大白鲨》的导演是Steven Spielberg。\n",
      "追问：Steven Spielberg来自哪里？\n",
      "中间答案：美国。\n",
      "追问：谁是《皇家赌场》的导演？\n",
      "中间答案：《皇家赌场》的导演是Martin Campbell。\n",
      "追问：Martin Campbell来自哪里？\n",
      "中间答案：新西兰。\n",
      "所以最终答案是：不是\n",
      "\n",
      "\n",
      "Question: 玛丽·波尔·华盛顿的父亲是谁?\n"
     ]
    }
   ],
   "source": [
    "# 导入 FewShotPromptTemplate 类\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# 创建一个 FewShotPromptTemplate 对象\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,           # 使用前面定义的 examples 作为范例\n",
    "    example_prompt=example_prompt, # 使用前面定义的 example_prompt 作为提示模板\n",
    "    suffix=\"Question: {input}\",    # 后缀模板，其中 {input} 会被替换为实际输入\n",
    "    input_variables=[\"input\"]     # 定义输入变量的列表\n",
    ")\n",
    "\n",
    "# 使用给定的输入格式化 prompt，并打印结果\n",
    "# 这里的 {input} 将被 \"玛丽·波尔·华盛顿的父亲是谁?\" 替换\n",
    "print(few_shot_prompt.format(input=\"玛丽·波尔·华盛顿的父亲是谁?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559cecd-ad58-495e-92ff-765233888ad8",
   "metadata": {},
   "source": [
    "## 示例选择器 Example Selectors\n",
    "\n",
    "**如果你有大量的参考示例，就得选择哪些要包含在提示中。最好还是根据某种条件或者规则来自动选择，Example Selector 是负责这个任务的类。**\n",
    "\n",
    "BaseExampleSelector 定义如下：\n",
    "\n",
    "```python\n",
    "class BaseExampleSelector(ABC):\n",
    "    \"\"\"用于选择包含在提示中的示例的接口。\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "        \"\"\"根据输入选择要使用的示例。\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "`ABC` 是 Python 中的 `abc` 模块中的一个缩写，它表示 \"Abstract Base Class\"（抽象基类）。在 Python 中，抽象基类用于定义其他类必须遵循的基本接口或蓝图，但不能直接实例化。其主要目的是为了提供一种形式化的方式来定义和检查子类的接口。\n",
    "\n",
    "使用抽象基类的几点关键信息：\n",
    "\n",
    "1. **抽象方法**：在抽象基类中，你可以定义抽象方法，它没有实现（也就是说，它没有方法体）。任何继承该抽象基类的子类都必须提供这些抽象方法的实现。\n",
    "\n",
    "2. **不能直接实例化**：你不能直接创建抽象基类的实例。试图这样做会引发错误。它们的主要目的是为了被继承，并在子类中实现其方法。\n",
    "\n",
    "3. **强制子类实现**：如果子类没有实现所有的抽象方法，那么试图实例化该子类也会引发错误。这确保了继承抽象基类的所有子类都遵循了预定的接口。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b028b-3762-4b3d-99ca-98c88b175677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8886dad-54b9-4fdf-8481-8abdd506676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的模块和类\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# 定义一个提示模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],     # 输入变量的名字\n",
    "    template=\"Input: {input}\\nOutput: {output}\",  # 实际的模板字符串\n",
    ")\n",
    "\n",
    "# 这是一个假设的任务示例列表，用于创建反义词\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77160159-3ae8-467e-a27c-a670952967c8",
   "metadata": {},
   "source": [
    "### Pandas 相关包首次导入错误后，再次执行即可正确导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e15785f7-fa11-4b2b-a668-9838b14839ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从给定的示例中创建一个语义相似性选择器\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,                          # 可供选择的示例列表\n",
    "    OpenAIEmbeddings(),                # 用于生成嵌入向量的嵌入类，用于衡量语义相似性\n",
    "    Chroma,                            # 用于存储嵌入向量并进行相似性搜索的 VectorStore 类\n",
    "    k=1                                # 要生成的示例数量\n",
    ")\n",
    "\n",
    "# 创建一个 FewShotPromptTemplate 对象\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # 提供一个 ExampleSelector 替代示例\n",
    "    example_prompt=example_prompt,      # 前面定义的提示模板\n",
    "    prefix=\"Give the antonym of every input\", # 前缀模板\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",     # 后缀模板\n",
    "    input_variables=[\"adjective\"],           # 输入变量的名字\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c24dd07-c3fc-4c85-9481-a6d163723b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# 输入是一种感受，所以应该选择 happy/sad 的示例。\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c4a113f-0719-4534-8932-1a363991da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: long\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# 输入是一种度量，所以应该选择 tall/short的示例。\n",
    "print(similar_prompt.format(adjective=\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "760abbb9-c00c-43fb-8c73-341c3993f720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: rain\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(similar_prompt.format(adjective=\"rain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0fda7-fbef-4ded-8eb5-2a482c0475d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
