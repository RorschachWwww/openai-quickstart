





from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import os

print(os.getenv('OPENAI_API_KEY'))
model = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("讲个关于 {topic} 的笑话吧")
chain = prompt | model





# 查看 Chain 的输入类型
chain.input_schema.schema()


# 查看 Prompt 的输入类型（Chain的输入从 Prompt 开始，因此输入类型一致）
prompt.input_schema.schema()


# 查看 Model 的输入类型
model.input_schema.schema()











# 查看 Chain 的输出类型
chain.output_schema.schema()





for s in chain.stream({"topic": "程序员"}):
    print(s.content, end="", flush=True)





chain.invoke({"topic": "程序员"})





chain.batch([{"topic": "程序员"}, {"topic": "产品经理"}, {"topic": "测试经理"}])


messages = chain.batch([{"topic": "程序员"}, {"topic": "产品经理"}, {"topic": "测试经理"}])


# 使用 StrOutputParser 来处理 Batch 批量输出
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()

for idx, m in enumerate(messages):
    print(f"笑话{idx}:\n")
    print(output_parser.invoke(m))
    print("\n")

















async for s in chain.astream({"topic": "程序员"}):
    print(s.content, end="", flush=True)





await chain.ainvoke({"topic": "程序员"})





await chain.abatch([{"topic": "程序员"}, {"topic": "产品经理"}, {"topic": "测试经理"}])



